{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gett Failed Order Insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, let's transform the response into a more narrative and report-like format:\n",
    "\n",
    "---\n",
    "\n",
    "**Data Analysis Report: Unsuccessful Orders Matching Metrics**\n",
    "\n",
    "*Prepared by [Your Name], [Your Position]*\n",
    "\n",
    "*Date: [Current Date]*\n",
    "\n",
    "---\n",
    "\n",
    "**Executive Summary:**\n",
    "\n",
    "This report delves into the matching metrics for unsuccessful orders in the context of the Gett platform. By merging the `data_orders` and `data_offers` datasets, we aim to extract meaningful insights to enhance the efficiency of order fulfillment.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Data Integration:**\n",
    "\n",
    "The initial step involves consolidating information from `data_orders` and `data_offers` through a seamless merge on the 'order_gk' column. This integration sets the foundation for a comprehensive analysis of order details and associated offers.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Unsuccessful Orders Subset:**\n",
    "\n",
    "A focused examination of unsuccessful orders is crucial. For this analysis, we concentrate on orders with a status of 4 (cancelled by the client) and 9 (cancelled by the system). This subset lays the groundwork for understanding the dynamics of unsuccessful order scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Matching Time Analysis:**\n",
    "\n",
    "To gauge the efficiency of the matching system, we calculate the time taken for both successful and unsuccessful orders to find a match. By comparing the distribution of matching times, we aim to uncover any notable disparities that could influence order fulfillment.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Driver Acceptance Rate:**\n",
    "\n",
    "Understanding the rate at which drivers are assigned to orders is pivotal. Analysis of the 'is_driver_assigned_key' column provides insights into the success rates of driver assignments for both successful and unsuccessful orders.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Geographical Analysis:**\n",
    "\n",
    "The geographical distribution of unsuccessful orders is explored to identify specific locations or regions that may be more prone to order failures. This analysis aims to uncover spatial patterns that could inform targeted improvements in service.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Cancellation Time Analysis:**\n",
    "\n",
    "An examination of the 'cancellation_time_in_seconds' for unsuccessful orders is conducted. This analysis seeks to identify any discernible patterns that might indicate quicker cancellations for specific orders, shedding light on potential areas for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Offer ID Analysis:**\n",
    "\n",
    "By investigating the association between offer IDs and unsuccessful orders, we aim to identify specific offers that are frequently associated with cancellations. This analysis provides insights into potential issues with certain offers.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Temporal Patterns:**\n",
    "\n",
    "Temporal patterns are explored to understand if there are specific times of the day or days of the week when unsuccessful orders are more prevalent. This temporal analysis informs us of potential peak demand hours or systemic issues during specific periods.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This comprehensive analysis of matching metrics for unsuccessful orders provides valuable insights into various facets of order fulfillment. The findings outlined in this report serve as a foundation for targeted improvements aimed at enhancing the overall efficiency and success rate of order assignments.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to customize the report as needed, and let me know if there's anything specific you'd like to focus on or if you have additional questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "PATH = '/Users/azamaufar/stratascratch/gettinsight/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Integration:**\n",
    "\n",
    "The initial step involves consolidating information from `data_orders` and `data_offers` through a seamless merge on the 'order_gk' column. This integration sets the foundation for a comprehensive analysis of order details and associated offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/azamaufar/stratascratch/gettinsight/dataset/data_orders.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/azamaufar/stratascratch/gettinsight/notebook/notebook.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/azamaufar/stratascratch/gettinsight/notebook/notebook.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the data from CSV files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/azamaufar/stratascratch/gettinsight/notebook/notebook.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_orders \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(PATH\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdataset/data_orders.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/azamaufar/stratascratch/gettinsight/notebook/notebook.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_offers \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(PATH\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset/data_offers.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/azamaufar/stratascratch/gettinsight/notebook/notebook.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Merge the datasets on the 'order_gk' column\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/azamaufar/stratascratch/gettinsight/dataset/data_orders.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data from CSV files\n",
    "data_orders = pd.read_csv(PATH+'dataset/data_orders.csv')\n",
    "data_offers = pd.read_csv(PATH+'dataset/data_offers.csv')\n",
    "\n",
    "# Merge the datasets on the 'order_gk' column\n",
    "merged_data = pd.merge(data_orders, data_offers, on='order_gk')\n",
    "\n",
    "# Display the merged dataset (optional)\n",
    "print(merged_data.head())\n",
    "\n",
    "# Save the merged dataset to a new CSV file (optional)\n",
    "merged_data.to_csv('path/to/merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
